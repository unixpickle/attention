package attention

import (
	"github.com/unixpickle/anydiff"
	"github.com/unixpickle/anydiff/anyseq"
	"github.com/unixpickle/anynet/anyrnn"
	"github.com/unixpickle/anyvec"
)

// A softBlock is an RNN block intended to be used for a
// soft-attention mechanism.
type softBlock struct {
	// Internal is the controlling RNN block.
	// It takes batches generated by ToInternal.
	Internal anyrnn.Block

	// Encoded is the processed and pooled sequence of
	// encoded vectors.
	Encoded anyseq.Seq

	// Attentor is used to satisfy queries.
	Attentor *SoftAttentor

	// InitQuery is the initial query.
	InitQuery *anydiff.Var

	// ToInternal produces an input for Internal given the
	// result of the previous timestep's queries and the
	// timestep's inputs.
	ToInternal func(queryRes, blockIn anydiff.Res, n int) anydiff.Res
}

func (s *softBlock) Start(n int) anyrnn.State {
	return &softBlockState{
		Internal: s.Internal.Start(n),
		Query:    anyrnn.NewVecState(s.InitQuery.Vector, n),
		V:        anydiff.NewVarSet(s.InitQuery),
	}
}

func (s *softBlock) PropagateStart(absSG anyrnn.StateGrad, g anydiff.Grad) {
	sg := absSG.(*softBlockStateGrad)
	s.Internal.PropagateStart(sg.Internal, g)
	sg.Query.PropagateStart(s.InitQuery, g)
}

func (s *softBlock) Step(absState anyrnn.State, in anyvec.Vector) anyrnn.Res {
	state := absState.(*softBlockState)
	inPool := anydiff.NewVar(in)
	queryPool := anydiff.NewVar(state.Query.Vector)
	queryRes := s.applyQueries(state.Present(), queryPool)
	blockIn := s.ToInternal(queryRes, inPool, state.Present().NumPresent())
	blockRes := s.Internal.Step(state.Internal, blockIn.Output())
	newState := &softBlockState{
		Internal: blockRes.State(),
		Query: &anyrnn.VecState{
			Vector:     blockRes.Output(),
			PresentMap: state.Present(),
		},
	}
	v := anydiff.MergeVarSets(blockIn.Vars(), blockRes.Vars(), state.V)
	v.Del(queryPool)
	v.Del(inPool)
	return &softBlockRes{
		QueryPool:   queryPool,
		InPool:      inPool,
		InternalIn:  blockIn,
		InternalRes: blockRes,
		NewState:    newState,
		V:           v,
	}
}

func (s *softBlock) applyQueries(pres anyrnn.PresentMap, query anydiff.Res) anydiff.Res {
	reducedSeqs := anyseq.Reduce(s.Encoded, pres)
	return anyseq.PoolToVec(reducedSeqs, func(reducedSeqs anyseq.Seq) anydiff.Res {
		appliedQuery := s.Attentor.QueryTrans.Apply(query, pres.NumPresent())
		return anydiff.Pool(appliedQuery, func(appliedQuery anydiff.Res) anydiff.Res {
			rawOuts := s.rawQueryOuts(reducedSeqs, pres, appliedQuery)
			exps := anyseq.Pool(rawOuts, func(rawOuts anyseq.Seq) anyseq.Seq {
				maxes := s.maxPerSeq(rawOuts)
				return s.exponentiate(rawOuts, maxes)
			})
			return anyseq.PoolToVec(exps, func(exps anyseq.Seq) anydiff.Res {
				masked := anyseq.MapN(func(n int, v ...anydiff.Res) anydiff.Res {
					mat := &anydiff.Matrix{
						Data: v[1],
						Rows: n,
						Cols: v[1].Output().Len() / n,
					}
					return anydiff.ScaleRows(mat, v[0]).Data
				}, exps, reducedSeqs)
				maskedSum := s.sums(masked)
				n := masked.Output()[0].NumPresent()
				sumMat := &anydiff.Matrix{
					Data: maskedSum,
					Rows: n,
					Cols: maskedSum.Output().Len() / n,
				}
				return anydiff.ScaleRows(sumMat, s.normalizers(exps)).Data
			})
		})
	})
}

func (s *softBlock) rawQueryOuts(reduced anyseq.Seq, pres anyrnn.PresentMap,
	transQuery anydiff.Res) anyseq.Seq {
	queryBlock := &anyrnn.FuncBlock{
		Func: func(in, state anydiff.Res, n int) (out, newState anydiff.Res) {
			x := anydiff.Add(transQuery, in)
			return nil, s.Attentor.OutTrans.Apply(x, n)
		},
		MakeStart: func(n int) anydiff.Res {
			if n != pres.NumPresent() {
				panic("bad state size")
			}
			return transQuery
		},
	}
	return anyrnn.Map(reduced, queryBlock)
}

func (s *softBlock) maxPerSeq(rawOuts anyseq.Seq) anyvec.Vector {
	maxBlock := &anyrnn.FuncBlock{
		Func: func(in, state anydiff.Res, n int) (out, newState anydiff.Res) {
			elemMax := in.Output().Copy()
			anyvec.ElemMax(elemMax, state.Output())
			return nil, anydiff.NewConst(elemMax)
		},
		MakeStart: func(n int) anydiff.Res {
			c := rawOuts.Output()[0].Packed.Creator()
			outs := c.MakeVector(n)
			// TODO: look into using -inf here.
			outs.AddScaler(c.MakeNumeric(-10000))
			return anydiff.NewConst(outs)
		},
	}
	return anyseq.Tail(anyrnn.Map(rawOuts, maxBlock)).Output()
}

func (s *softBlock) exponentiate(rawOuts anyseq.Seq, maxes anyvec.Vector) anyseq.Seq {
	expBlock := &anyrnn.FuncBlock{
		Func: func(in, state anydiff.Res, n int) (out, newState anydiff.Res) {
			return nil, anydiff.Exp(anydiff.Sub(in, state))
		},
		MakeStart: func(n int) anydiff.Res {
			if n != maxes.Len() {
				panic("bad state size")
			}
			return anydiff.NewConst(maxes)
		},
	}
	return anyrnn.Map(rawOuts, expBlock)
}

func (s *softBlock) normalizers(exps anyseq.Seq) anydiff.Res {
	sum := s.sums(exps)
	c := sum.Output().Creator()
	ones := c.MakeVector(sum.Output().Len())
	ones.AddScaler(c.MakeNumeric(1))
	return anydiff.Div(anydiff.NewConst(ones), sum)
}

func (s *softBlock) sums(seqs anyseq.Seq) anydiff.Res {
	sumBlock := &anyrnn.FuncBlock{
		Func: func(in, state anydiff.Res, n int) (out, newState anydiff.Res) {
			return nil, anydiff.Add(in, state)
		},
		MakeStart: func(n int) anydiff.Res {
			firstOut := seqs.Output()[0]
			c := firstOut.Packed.Creator()
			inSize := firstOut.Packed.Len() / firstOut.NumPresent()
			return anydiff.NewConst(c.MakeVector(n * inSize))
		},
	}
	return anyseq.Tail(anyrnn.Map(seqs, sumBlock))
}

type softBlockState struct {
	Internal anyrnn.State
	Query    *anyrnn.VecState
	V        anydiff.VarSet
}

func (s *softBlockState) Present() anyrnn.PresentMap {
	return s.Internal.Present()
}

func (s *softBlockState) Reduce(p anyrnn.PresentMap) anyrnn.State {
	return &softBlockState{
		Internal: s.Internal.Reduce(p),
		Query:    s.Query.Reduce(p).(*anyrnn.VecState),
		V:        s.V,
	}
}

type softBlockStateGrad struct {
	Internal anyrnn.StateGrad
	Query    *anyrnn.VecState
}

func (s *softBlockStateGrad) Present() anyrnn.PresentMap {
	return s.Internal.Present()
}

func (s *softBlockStateGrad) Expand(p anyrnn.PresentMap) anyrnn.StateGrad {
	return &softBlockStateGrad{
		Internal: s.Internal.Expand(p),
		Query:    s.Query.Expand(p).(*anyrnn.VecState),
	}
}

type softBlockRes struct {
	QueryPool   *anydiff.Var
	InPool      *anydiff.Var
	InternalIn  anydiff.Res
	InternalRes anyrnn.Res

	NewState *softBlockState
	V        anydiff.VarSet
}

func (s *softBlockRes) State() anyrnn.State {
	return s.NewState
}

func (s *softBlockRes) Output() anyvec.Vector {
	return s.InternalRes.Output()
}

func (s *softBlockRes) Vars() anydiff.VarSet {
	return s.V
}

func (s *softBlockRes) Propagate(u anyvec.Vector, sg anyrnn.StateGrad,
	g anydiff.Grad) (anyvec.Vector, anyrnn.StateGrad) {
	var internalSG anyrnn.StateGrad
	if s != nil {
		sg := sg.(*softBlockStateGrad)
		u.Add(sg.Query.Vector)
		internalSG = sg.Internal
	}

	inDown, stateDown := s.InternalRes.Propagate(u, internalSG, g)

	for _, v := range []*anydiff.Var{s.InPool, s.QueryPool} {
		g[v] = v.Output().Creator().MakeVector(v.Output().Len())
	}
	s.InternalIn.Propagate(inDown, g)

	trueDown := g[s.InPool]
	queryDown := g[s.QueryPool]
	delete(g, s.InPool)
	delete(g, s.QueryPool)

	return trueDown, &softBlockStateGrad{
		Internal: stateDown,
		Query: &anyrnn.VecState{
			Vector:     queryDown,
			PresentMap: s.NewState.Present(),
		},
	}
}
